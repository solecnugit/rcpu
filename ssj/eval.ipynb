{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import random \n",
    "import os\n",
    "import ujson\n",
    "\n",
    "from typing import List, Dict, Tuple, Union, Any\n",
    "\n",
    "def set_seed():\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    \n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icx_df = pd.read_parquet(\"out/icx.parquet\")\n",
    "clx_df = pd.read_parquet(\"out/clx.parquet\")\n",
    "\n",
    "icx_ssj = ujson.load(open(\"out/ssj.icx.json\"))\n",
    "clx_ssj = ujson.load(open(\"out/ssj.clx.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_df(df: pd.DataFrame):\n",
    "    df = df[\n",
    "        [\n",
    "            \"timestamp\",\n",
    "            \"%cpu_lc0\",\n",
    "            \"%cpu_lc1\",\n",
    "            \"cpu_lc0\",\n",
    "            \"cpu_lc1\",\n",
    "            \"phy_cpu\",\n",
    "            \"name\",\n",
    "            \"%cpu_phy\",\n",
    "        ]\n",
    "    ]\n",
    "    df_lc0 = df[[\"timestamp\", \"%cpu_lc0\", \"cpu_lc0\", \"phy_cpu\", \"%cpu_phy\", \"name\"]]\n",
    "    df_lc1 = df[[\"timestamp\", \"%cpu_lc1\", \"cpu_lc1\", \"phy_cpu\", \"%cpu_phy\", \"name\"]]\n",
    "\n",
    "    df_lc1 = df_lc1.rename(columns={\"%cpu_lc1\": \"%cpu_lc0\", \"cpu_lc1\": \"cpu_lc0\"})\n",
    "\n",
    "    df = pd.concat([df_lc0, df_lc1])\n",
    "    df = df.sort_values(by=[\"timestamp\"])\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df.rename(\n",
    "        columns={\n",
    "            \"%cpu_lc0\": \"%cpu\",\n",
    "            \"cpu_lc0\": \"cpu\",\n",
    "            \"phy_cpu\": \"phy\",\n",
    "            \"%cpu_phy\": \"%cpu_phy\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    cpu_df = df.pivot(index=[\"name\", \"timestamp\"], columns=\"cpu\", values=\"%cpu\")\n",
    "\n",
    "    phy_df = (\n",
    "        df.groupby([\"name\", \"timestamp\"])\n",
    "        .agg({\"%cpu_phy\": \"mean\"})\n",
    "        .rename(columns={\"%cpu_phy\": \"physical\"})\n",
    "    )\n",
    "\n",
    "    df = cpu_df.join(phy_df, on=[\"name\", \"timestamp\"], how=\"inner\")\n",
    "    df = df.reset_index()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot data into row format\n",
    "icx_df = pivot_df(icx_df)\n",
    "clx_df = pivot_df(clx_df)\n",
    "\n",
    "# calculate remaining utilization\n",
    "icx_df[\"baseline\"] = icx_df.drop(columns=[\"name\", \"timestamp\", \"physical\"]).mean(axis=1)\n",
    "icx_df[\"remaining(baseline)\"] = 100 - icx_df[\"baseline\"]\n",
    "\n",
    "clx_df[\"baseline\"] = clx_df.drop(columns=[\"name\", \"timestamp\", \"physical\"]).mean(axis=1)\n",
    "clx_df[\"remaining(baseline)\"] = 100 - clx_df[\"baseline\"]\n",
    "\n",
    "cols = []\n",
    "tdf = icx_df.drop(\n",
    "    columns=[\"name\", \"timestamp\", \"physical\", \"baseline\", \"remaining(baseline)\"]\n",
    ")\n",
    "\n",
    "for a, b in tdf.columns.groupby(tdf.columns % 32).values():\n",
    "    tdf[f\"p{a}\"] = tdf[[a, b]].max(axis=1)\n",
    "    cols.append(f\"p{a}\")\n",
    "\n",
    "icx_df[\"adjusted\"] = tdf[cols].mean(axis=1)\n",
    "icx_df[\"remaining(rcpu)\"] = 100 - icx_df[\"adjusted\"]\n",
    "\n",
    "cols = []\n",
    "tdf = clx_df.drop(\n",
    "    columns=[\"name\", \"timestamp\", \"physical\", \"baseline\", \"remaining(baseline)\"]\n",
    ")\n",
    "\n",
    "for a, b in tdf.columns.groupby(tdf.columns % 40).values():\n",
    "    tdf[f\"p{a}\"] = tdf[[a, b]].max(axis=1)\n",
    "    cols.append(f\"p{a}\")\n",
    "\n",
    "clx_df[\"adjusted\"] = tdf[cols].mean(axis=1)\n",
    "\n",
    "clx_df[\"remaining(rcpu)\"] = 100 - clx_df[\"adjusted\"]\n",
    "\n",
    "icx_df[\"remaining(physical)\"] = 100 - icx_df[\"physical\"]\n",
    "clx_df[\"remaining(physical)\"] = 100 - clx_df[\"physical\"]\n",
    "\n",
    "icx_df = icx_df.reset_index(drop=True)\n",
    "clx_df = clx_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split dataset\n",
    "\n",
    "cols_to_drop_when_x = [\n",
    "    \"name\",\n",
    "    \"timestamp\",\n",
    "    \"physical\",\n",
    "    \"baseline\",\n",
    "    \"adjusted\",\n",
    "    \"remaining(baseline)\",\n",
    "    \"remaining(rcpu)\",\n",
    "    \"remaining(physical)\",\n",
    "]\n",
    "\n",
    "ICX_X = icx_df.drop(columns=cols_to_drop_when_x)\n",
    "CLX_X = clx_df.drop(columns=cols_to_drop_when_x)\n",
    "\n",
    "# padding features to 40\n",
    "ICX_X.columns = [i for i in range(32)]\n",
    "for i in range(32, 40):\n",
    "    ICX_X[i] = 0\n",
    "\n",
    "CLX_X.columns = [i for i in range(40)]\n",
    "\n",
    "ICX_y = icx_df[\"remaining(physical)\"]\n",
    "CLX_y = clx_df[\"remaining(physical)\"]\n",
    "\n",
    "icx_X_train, icx_X_test, icx_y_train, icx_y_test = train_test_split(\n",
    "    ICX_X, ICX_y, test_size=0.2, random_state=42\n",
    ")\n",
    "clx_X_train, clx_X_test, clx_y_train, clx_y_test = train_test_split(\n",
    "    CLX_X, CLX_y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "import time\n",
    "\n",
    "GLOBAL_ERRORS = []\n",
    "GLOBAL_SPEED = []\n",
    "\n",
    "RANDOM_INPUT_ARRAY = np.random.random((10000, 40)) * 100 # for speed test\n",
    "\n",
    "# Evaluation functions\n",
    "# NOTE: The utilization data in our dataset is already normalized to 100.\n",
    "#       Thus, the error mentioned in our evaluation is in percentage.\n",
    "#       We report the raw value in our paper\n",
    "def eval_on_icx(model, train_on):\n",
    "    model_name = model.__class__.__name__\n",
    "    eval_on = \"ICX\"\n",
    "\n",
    "    y_pred = model.predict(icx_X_test)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(icx_y_test, y_pred))\n",
    "    mae = mean_absolute_error(icx_y_test, y_pred)\n",
    "\n",
    "    GLOBAL_ERRORS.append(\n",
    "        {\n",
    "            \"model\": model_name,\n",
    "            \"train\": train_on,\n",
    "            \"eval\": eval_on,\n",
    "            \"rmse\": rmse,\n",
    "            \"mae\": mae,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "def eval_on_clx(model, train_on):\n",
    "    model_name = model.__class__.__name__\n",
    "    eval_on = \"CLX\"\n",
    "\n",
    "    y_pred = model.predict(clx_X_test)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(clx_y_test, y_pred))\n",
    "    mae = mean_absolute_error(clx_y_test, y_pred)\n",
    "\n",
    "    GLOBAL_ERRORS.append(\n",
    "        {\n",
    "            \"model\": model_name,\n",
    "            \"train\": train_on,\n",
    "            \"eval\": eval_on,\n",
    "            \"rmse\": rmse,\n",
    "            \"mae\": mae,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "def eval_speed(model):\n",
    "    model_name = model.__class__.__name__\n",
    "    times = []\n",
    "\n",
    "    # warm up\n",
    "    for _ in range(10):\n",
    "        model.predict(RANDOM_INPUT_ARRAY)\n",
    "\n",
    "    for _ in range(20):\n",
    "        start = time.clock_gettime_ns(time.CLOCK_MONOTONIC)\n",
    "        model.predict(RANDOM_INPUT_ARRAY) # Shape: (10000, 40)\n",
    "        end = time.clock_gettime_ns(time.CLOCK_MONOTONIC)\n",
    "\n",
    "        times.append(end - start)\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    GLOBAL_SPEED.append(\n",
    "        {\n",
    "            \"model\": model_name,\n",
    "            \"time\": np.mean(times),\n",
    "            \"times\": times,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from hyperopt import STATUS_OK, hp, tpe, Trials, fmin\n",
    "\n",
    "\n",
    "def icx_objective(space):\n",
    "    reg = create_model(space) # see function definition below\n",
    "\n",
    "    score = cross_val_score(\n",
    "        reg,\n",
    "        icx_X_train,\n",
    "        icx_y_train,\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        scoring=\"neg_root_mean_squared_error\",\n",
    "    ).mean()\n",
    "\n",
    "    return {\"loss\": -score, \"status\": STATUS_OK}\n",
    "\n",
    "\n",
    "def clx_objective(space):\n",
    "    reg = create_model(space)\n",
    "\n",
    "    score = cross_val_score(\n",
    "        reg,\n",
    "        clx_X_train,\n",
    "        clx_y_train,\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        scoring=\"neg_root_mean_squared_error\",\n",
    "    ).mean()\n",
    "\n",
    "    return {\"loss\": -score, \"status\": STATUS_OK}\n",
    "\n",
    "\n",
    "EVAL_TURNS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "\n",
    "\n",
    "def create_model(space):\n",
    "    return xgb.XGBRegressor(\n",
    "        random_state=42,\n",
    "        verbosity=0,\n",
    "        n_estimators=int(space[\"n_estimators\"]),\n",
    "        max_depth=int(space[\"max_depth\"]),\n",
    "        gamma=space[\"gamma\"],\n",
    "        reg_alpha=int(space[\"reg_alpha\"]),\n",
    "        min_child_weight=space[\"min_child_weight\"],\n",
    "        colsample_bytree=space[\"colsample_bytree\"],\n",
    "    )\n",
    "\n",
    "\n",
    "space = {\n",
    "    \"max_depth\": hp.quniform(\"max_depth\", 3, 18, 1),\n",
    "    \"gamma\": hp.uniform(\"gamma\", 1, 9),\n",
    "    \"reg_alpha\": hp.quniform(\"reg_alpha\", 40, 180, 1),\n",
    "    \"reg_lambda\": hp.uniform(\"reg_lambda\", 0, 1),\n",
    "    \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.5, 1),\n",
    "    \"min_child_weight\": hp.quniform(\"min_child_weight\", 0, 10, 1),\n",
    "    \"n_estimators\": hp.uniform(\"n_estimators\", 100, 1000),\n",
    "}\n",
    "\n",
    "trials = Trials()\n",
    "icx_best_params = fmin(\n",
    "    icx_objective, space, algo=tpe.suggest, max_evals=EVAL_TURNS, trials=trials\n",
    ")\n",
    "\n",
    "trials = Trials()\n",
    "clx_best_params = fmin(\n",
    "    clx_objective, space, algo=tpe.suggest, max_evals=EVAL_TURNS, trials=trials\n",
    ")\n",
    "\n",
    "icx_model = create_model(icx_best_params)\n",
    "clx_model = create_model(clx_best_params)\n",
    "\n",
    "icx_model.fit(icx_X_train, icx_y_train)\n",
    "clx_model.fit(clx_X_train, clx_y_train)\n",
    "\n",
    "eval_on_icx(icx_model, \"ICX\")\n",
    "eval_on_clx(clx_model, \"CLX\")\n",
    "\n",
    "eval_speed(icx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "set_seed()\n",
    "\n",
    "\n",
    "def create_model(space):\n",
    "    return KNeighborsRegressor(\n",
    "        n_neighbors=int(space[\"n_neighbors\"]),\n",
    "        #    weights=space[\"weights\"],\n",
    "        #    algorithm=space[\"algorithm\"],\n",
    "        leaf_size=int(space[\"leaf_size\"]),\n",
    "        p=int(space[\"p\"]),\n",
    "    )\n",
    "\n",
    "\n",
    "space = {\n",
    "    \"n_neighbors\": hp.uniform(\"n_neighbors\", 1, 100),\n",
    "    # \"weights\": hp.choice(\"weights\", [\"uniform\", \"distance\"]),\n",
    "    # \"algorithm\": hp.choice(\"algorithm\", [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"]),\n",
    "    \"leaf_size\": hp.uniform(\"leaf_size\", 1, 100),\n",
    "    \"p\": hp.uniform(\"p\", 1, 10),\n",
    "}\n",
    "\n",
    "trials = Trials()\n",
    "icx_best_params = fmin(\n",
    "    icx_objective, space, algo=tpe.suggest, max_evals=EVAL_TURNS, trials=trials\n",
    ")\n",
    "\n",
    "trials = Trials()\n",
    "clx_best_params = fmin(\n",
    "    clx_objective, space, algo=tpe.suggest, max_evals=EVAL_TURNS, trials=trials\n",
    ")\n",
    "\n",
    "icx_model = create_model(icx_best_params)\n",
    "clx_model = create_model(clx_best_params)\n",
    "\n",
    "icx_model.fit(icx_X_train, icx_y_train)\n",
    "clx_model.fit(clx_X_train, clx_y_train)\n",
    "\n",
    "eval_on_icx(icx_model, \"ICX\")\n",
    "eval_on_clx(clx_model, \"CLX\")\n",
    "\n",
    "eval_speed(icx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "set_seed()\n",
    "\n",
    "\n",
    "def create_model(space):\n",
    "    return LassoCV(\n",
    "        cv=int(space[\"cv\"]),\n",
    "        n_alphas=int(space[\"n_alphas\"]),\n",
    "        tol=space[\"tol\"],\n",
    "        max_iter=10000,\n",
    "    )\n",
    "\n",
    "\n",
    "space = {\n",
    "    \"cv\": hp.uniform(\"cv\", 2, 10),\n",
    "    \"n_alphas\": hp.uniform(\"n_alphas\", 1, 100),\n",
    "    \"tol\": hp.uniform(\"tol\", 0.0001, 0.001),\n",
    "    # \"max_iter\": hp.uniform(\"max_iter\", 100, 3000),\n",
    "}\n",
    "\n",
    "trials = Trials()\n",
    "icx_best_params = fmin(\n",
    "    icx_objective, space, algo=tpe.suggest, max_evals=EVAL_TURNS, trials=trials\n",
    ")\n",
    "\n",
    "trials = Trials()\n",
    "clx_best_params = fmin(\n",
    "    clx_objective, space, algo=tpe.suggest, max_evals=EVAL_TURNS, trials=trials\n",
    ")\n",
    "\n",
    "icx_model = create_model(icx_best_params)\n",
    "clx_model = create_model(clx_best_params)\n",
    "\n",
    "icx_model.fit(icx_X_train, icx_y_train)\n",
    "clx_model.fit(clx_X_train, clx_y_train)\n",
    "\n",
    "eval_on_icx(icx_model, \"ICX\")\n",
    "\n",
    "eval_on_clx(clx_model, \"CLX\")\n",
    "\n",
    "eval_speed(icx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "set_seed()\n",
    "\n",
    "\n",
    "def create_model(space):\n",
    "    return SVR(\n",
    "        # kernel=space[\"kernel\"],\n",
    "        degree=int(space[\"degree\"]),\n",
    "        gamma=space[\"gamma\"],\n",
    "        tol=space[\"tol\"],\n",
    "        C=space[\"C\"],\n",
    "        epsilon=space[\"epsilon\"],\n",
    "    )\n",
    "\n",
    "\n",
    "space = {\n",
    "    # \"kernel\": hp.choice(\"kernel\", [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]),\n",
    "    \"degree\": hp.uniform(\"degree\", 1, 10),\n",
    "    \"gamma\": hp.uniform(\"gamma\", 0.0001, 1),\n",
    "    \"tol\": hp.uniform(\"tol\", 0.0001, 0.001),\n",
    "    \"C\": hp.uniform(\"C\", 0.1, 100),\n",
    "    \"epsilon\": hp.uniform(\"epsilon\", 0.0001, 0.1),\n",
    "}\n",
    "\n",
    "trials = Trials()\n",
    "icx_best_params = fmin(\n",
    "    icx_objective, space, algo=tpe.suggest, max_evals=30, trials=trials\n",
    ")\n",
    "\n",
    "trials = Trials()\n",
    "clx_best_params = fmin(\n",
    "    clx_objective, space, algo=tpe.suggest, max_evals=30, trials=trials\n",
    ")\n",
    "\n",
    "icx_model = create_model(icx_best_params)\n",
    "clx_model = create_model(clx_best_params)\n",
    "\n",
    "icx_model.fit(icx_X_train, icx_y_train)\n",
    "clx_model.fit(clx_X_train, clx_y_train)\n",
    "\n",
    "eval_on_icx(icx_model, \"ICX\")\n",
    "eval_on_clx(clx_model, \"CLX\")\n",
    "\n",
    "eval_speed(icx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "set_seed()\n",
    "\n",
    "\n",
    "def create_model(space):\n",
    "    return MLPRegressor(\n",
    "        hidden_layer_sizes=(\n",
    "            int(space[\"hidden_layer_sizes1\"]),\n",
    "            int(space[\"hidden_layer_sizes2\"]),\n",
    "        ),\n",
    "        learning_rate_init=space[\"learning_rate\"],\n",
    "        max_iter=10000,\n",
    "        alpha=space[\"alpha\"],\n",
    "        momentum=space[\"momentum\"],\n",
    "        beta_1=space[\"beta_1\"],\n",
    "        beta_2=space[\"beta_2\"],\n",
    "        epsilon=space[\"epsilon\"],\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "\n",
    "space = {\n",
    "    \"hidden_layer_sizes1\": hp.uniform(\"hidden_layer_sizes1\", 32, 128),\n",
    "    \"hidden_layer_sizes2\": hp.uniform(\"hidden_layer_sizes2\", 32, 128),\n",
    "    \"learning_rate\": hp.uniform(\"learning_rate\", 0.0001, 0.1),\n",
    "    # \"max_iter\": hp.uniform(\"max_iter\", 100, 1000),\n",
    "    \"alpha\": hp.uniform(\"alpha\", 0.0001, 0.1),\n",
    "    \"momentum\": hp.uniform(\"momentum\", 0.0001, 0.1),\n",
    "    \"beta_1\": hp.uniform(\"beta_1\", 0.0001, 0.1),\n",
    "    \"beta_2\": hp.uniform(\"beta_2\", 0.0001, 0.1),\n",
    "    \"epsilon\": hp.uniform(\"epsilon\", 0.0001, 0.1),\n",
    "}\n",
    "\n",
    "trials = Trials()\n",
    "icx_best_params = fmin(\n",
    "    icx_objective, space, algo=tpe.suggest, max_evals=30, trials=trials\n",
    ")\n",
    "\n",
    "trials = Trials()\n",
    "clx_best_params = fmin(\n",
    "    clx_objective, space, algo=tpe.suggest, max_evals=30, trials=trials\n",
    ")\n",
    "\n",
    "icx_model = create_model(icx_best_params)\n",
    "clx_model = create_model(clx_best_params)\n",
    "\n",
    "icx_model.fit(icx_X_train, icx_y_train)\n",
    "clx_model.fit(clx_X_train, clx_y_train)\n",
    "\n",
    "eval_on_icx(icx_model, \"ICX\")\n",
    "eval_on_clx(clx_model, \"CLX\")\n",
    "\n",
    "eval_speed(icx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval baseline\n",
    "\n",
    "y_pred = icx_df.iloc[icx_X_test.index][\"remaining(baseline)\"]\n",
    "\n",
    "rmse = mean_squared_error(icx_y_test, y_pred, squared=False)\n",
    "mae = mean_absolute_error(icx_y_test, y_pred)\n",
    "\n",
    "GLOBAL_ERRORS.append(\n",
    "    {\n",
    "        \"model\": \"Baseline\",\n",
    "        \"train\": \"ICX\",\n",
    "        \"eval\": \"ICX\",\n",
    "        \"rmse\": rmse,\n",
    "        \"mae\": mae,\n",
    "    }\n",
    ")\n",
    "\n",
    "y_pred = clx_df.iloc[clx_X_test.index][\"remaining(baseline)\"]\n",
    "\n",
    "rmse = mean_squared_error(clx_y_test, y_pred, squared=False)\n",
    "mae = mean_absolute_error(clx_y_test, y_pred)\n",
    "\n",
    "GLOBAL_ERRORS.append(\n",
    "    {\n",
    "        \"model\": \"Baseline\",\n",
    "        \"train\": \"CLX\",\n",
    "        \"eval\": \"CLX\",\n",
    "        \"rmse\": rmse,\n",
    "        \"mae\": mae,\n",
    "    }\n",
    ")\n",
    "\n",
    "# speed\n",
    "\n",
    "model_name = \"Baseline\"\n",
    "times = []\n",
    "\n",
    "for i in range(20):\n",
    "    start = time.clock_gettime_ns(time.CLOCK_MONOTONIC)\n",
    "    RANDOM_INPUT_ARRAY.mean(axis=1)\n",
    "    end = time.clock_gettime_ns(time.CLOCK_MONOTONIC)\n",
    "\n",
    "    times.append(end - start)\n",
    "    time.sleep(0.5)\n",
    "\n",
    "GLOBAL_SPEED.append(\n",
    "    {\n",
    "        \"model\": model_name,\n",
    "        \"time\": np.mean(times),\n",
    "        \"times\": times,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval adjusted\n",
    "\n",
    "y_pred = icx_df.iloc[icx_X_test.index][\"remaining(rcpu)\"]\n",
    "\n",
    "rmse = mean_squared_error(icx_y_test, y_pred, squared=False)\n",
    "mae = mean_absolute_error(icx_y_test, y_pred)\n",
    "\n",
    "GLOBAL_ERRORS.append(\n",
    "    {\n",
    "        \"model\": \"RCPU\",\n",
    "        \"train\": \"ICX\",\n",
    "        \"eval\": \"ICX\",\n",
    "        \"rmse\": rmse,\n",
    "        \"mae\": mae,\n",
    "    }\n",
    ")\n",
    "\n",
    "y_pred = clx_df.iloc[clx_X_test.index][\"remaining(rcpu)\"]\n",
    "\n",
    "rmse = mean_squared_error(clx_y_test, y_pred, squared=False)\n",
    "mae = mean_absolute_error(clx_y_test, y_pred)\n",
    "\n",
    "GLOBAL_ERRORS.append(\n",
    "    {\n",
    "        \"model\": \"RCPU\",\n",
    "        \"train\": \"CLX\",\n",
    "        \"eval\": \"CLX\",\n",
    "        \"rmse\": rmse,\n",
    "        \"mae\": mae,\n",
    "    }\n",
    ")\n",
    "\n",
    "# speed\n",
    "model_name = \"RCPU\"\n",
    "times = []\n",
    "\n",
    "for i in range(20):\n",
    "    start = time.clock_gettime_ns(time.CLOCK_MONOTONIC)\n",
    "\n",
    "    # Take the maximum of every 2 hardware threads\n",
    "    # The topology follows Intel Ice Lake\n",
    "    # 0,20 is on the same physical core and 1,21 is on the same physical core ...\n",
    "    t = np.maximum(RANDOM_INPUT_ARRAY[:, :20], RANDOM_INPUT_ARRAY[:, 20:])\n",
    "    t = np.mean(t, axis=1)\n",
    "\n",
    "    end = time.clock_gettime_ns(time.CLOCK_MONOTONIC)\n",
    "\n",
    "    times.append(end - start)\n",
    "\n",
    "    time.sleep(0.5)\n",
    "\n",
    "GLOBAL_SPEED.append(\n",
    "    {\n",
    "        \"model\": model_name,\n",
    "        \"time\": np.mean(times),\n",
    "        \"times\": times,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = pd.DataFrame.from_records(GLOBAL_ERRORS)\n",
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed = pd.DataFrame.from_records(GLOBAL_SPEED)\n",
    "speed[\"speed\"] = speed[\"times\"].apply(np.mean)\n",
    "speed = speed[[\"model\", \"speed\"]]\n",
    "\n",
    "baseline = speed.loc[speed.model == \"Baseline\", \"speed\"].item()\n",
    "speed[\"relative\"] = speed[\"speed\"] / baseline\n",
    "speed = speed.sort_values(\"relative\")\n",
    "speed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
